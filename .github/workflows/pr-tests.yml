name: PR Tests

on:
  pull_request:
    branches: [main, dev]
    types: [opened, synchronize, reopened]
  push:
    branches: [dev]

permissions:
  contents: read
  pull-requests: write
  issues: write
  checks: write

jobs:
  test:
    name: Run Tests
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        python-version: ["3.12"]
    
    # Removed PostgreSQL service - using SQLite for CI tests to avoid Docker Hub issues
    # services:
    #   postgres:
    #     image: postgres:15-alpine
    #     ...

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for better analysis

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            build-essential \
            libpq-dev \
            python3-dev

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -e .
          pip install -e ".[discord]"  # Install optional discord dependencies

      - name: Set up environment variables
        run: |
          # Use SQLite for CI tests to avoid Docker Hub authentication issues
          mkdir -p data
          echo "DATABASE_URL=sqlite:///data/test.db" >> $GITHUB_ENV
          echo "AUTOMAGIK_OMNI_API_KEY=${{ secrets.TEST_API_KEY || 'dummy-key' }}" >> $GITHUB_ENV
          echo "PYTHONPATH=$GITHUB_WORKSPACE" >> $GITHUB_ENV
          echo "CI=true" >> $GITHUB_ENV

      - name: Initialize database
        run: |
          python -c "
          from sqlalchemy import create_engine
          from src.db.models import Base
          import os
          # Create SQLite database
          engine = create_engine(os.environ['DATABASE_URL'])
          Base.metadata.create_all(engine)
          print('SQLite database initialized successfully')
          "

      - name: Run tests with coverage
        id: test
        run: |
          # Run tests with coverage, skip known CI issues
          # Note: Removed parallel execution (-n auto) as it can cause issues in CI
          pytest tests/ \
            -v \
            --tb=short \
            --color=yes \
            --cov=src \
            --cov-report=term-missing:skip-covered \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term > pytest-coverage.txt \
            --junitxml=test-results.xml \
            --maxfail=5 \
            --disable-warnings \
            -k "not test_bearer_token_validation and not test_add_command_help" \
            || TEST_FAILED=$?

          # Store test result
          if [ "${TEST_FAILED}" != "" ]; then
            echo "test_failed=true" >> $GITHUB_OUTPUT
            exit 1
          else
            echo "test_failed=false" >> $GITHUB_OUTPUT
          fi

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.python-version }}
          path: |
            test-results.xml
            htmlcov/
            pytest-coverage.txt
            coverage.xml
          retention-days: 7

      - name: Get target branch coverage for comparison
        if: github.event_name == 'pull_request' && always()
        id: target-coverage
        run: |
          # Get the target branch name (usually 'main' or 'dev')
          TARGET_BRANCH="${{ github.base_ref }}"
          echo "TARGET_BRANCH=$TARGET_BRANCH" >> $GITHUB_OUTPUT

          # Try to get coverage from target branch
          echo "Attempting to get coverage from target branch: $TARGET_BRANCH"

          # Check if we're in a git repository context
          if [ -d .git ]; then
            # Store current branch
            CURRENT_BRANCH=$(git rev-parse --abbrev-ref HEAD)

            # Fetch target branch
            git fetch origin $TARGET_BRANCH:$TARGET_BRANCH || echo "Could not fetch target branch"

            # Try to get target branch coverage by checking out and running tests
            if git checkout $TARGET_BRANCH 2>/dev/null; then
              echo "Successfully checked out target branch $TARGET_BRANCH"

              # Quick coverage check on target branch (only if deps are already installed)
              if pytest tests/ --cov=src --cov-report=term --maxfail=1 -q -x &>/dev/null; then
                TARGET_COVERAGE=$(pytest tests/ --cov=src --cov-report=term 2>/dev/null | grep -oP 'TOTAL.*?(\d+)%' | grep -oP '\d+%' | head -1 || echo "0%")
                echo "TARGET_COVERAGE=$TARGET_COVERAGE" >> $GITHUB_OUTPUT
                echo "Target branch coverage: $TARGET_COVERAGE"
              else
                echo "TARGET_COVERAGE=unknown" >> $GITHUB_OUTPUT
                echo "Could not determine target branch coverage"
              fi

              # Return to current branch
              git checkout $CURRENT_BRANCH
            else
              echo "TARGET_COVERAGE=unknown" >> $GITHUB_OUTPUT
              echo "Could not checkout target branch for coverage comparison"
            fi
          else
            # Not in a git context, use baseline
            echo "Not in git repository, using baseline coverage"
            echo "TARGET_COVERAGE=85%" >> $GITHUB_OUTPUT
          fi

      - name: Beautiful Coverage Comment
        if: github.event_name == 'pull_request' && always()
        uses: MishaKav/pytest-coverage-comment@main
        with:
          pytest-coverage-path: ./pytest-coverage.txt
          junitxml-path: ./test-results.xml
          unique-id-for-comment: coverage-${{ matrix.python-version }}
          title: "ğŸ“Š Coverage Report (Python ${{ matrix.python-version }}) vs ${{ steps.target-coverage.outputs.TARGET_BRANCH }}"
          badge-title: "Coverage"
          hide-badge: false
          hide-report: false
          create-new-comment: false
          hide-comment: false
          report-only-changed-files: false
          remove-link-from-badge: false

      - name: Coverage Comparison Summary
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            // Get current coverage from pytest output
            let currentCoverage = 'unknown';
            let coverageDetails = '';

            try {
              if (fs.existsSync('pytest-coverage.txt')) {
                const coverageContent = fs.readFileSync('pytest-coverage.txt', 'utf8');
                const coverageMatch = coverageContent.match(/TOTAL.*?(\d+)%/);
                if (coverageMatch) {
                  currentCoverage = coverageMatch[1];
                }

                // Get file-level coverage for significant changes
                const lines = coverageContent.split('\n');
                const fileLines = lines.filter(line =>
                  line.includes('.py') &&
                  !line.startsWith('Name') &&
                  !line.startsWith('TOTAL') &&
                  line.trim().length > 0
                );

                if (fileLines.length > 0) {
                  const significantFiles = fileLines
                    .map(line => {
                      const parts = line.trim().split(/\s+/);
                      if (parts.length >= 4) {
                        const fileName = parts[0];
                        const coverage = parts[parts.length - 1];
                        return { fileName, coverage };
                      }
                      return null;
                    })
                    .filter(file => file && !file.coverage.includes('%'))
                    .slice(0, 5); // Show top 5 files

                  if (significantFiles.length > 0) {
                    coverageDetails = '\n### ğŸ“ Key Files Coverage:\n' +
                      significantFiles.map(file =>
                        `- \`${file.fileName}\`: ${file.coverage}`
                      ).join('\n');
                  }
                }
              }
            } catch (e) {
              console.log('Could not parse coverage details:', e);
            }

            const targetCoverage = '${{ steps.target-coverage.outputs.TARGET_COVERAGE }}';
            const targetBranch = '${{ steps.target-coverage.outputs.TARGET_BRANCH }}';

            let comparisonMessage = '';
            let changeEmoji = 'ğŸ“Š';

            if (targetCoverage && targetCoverage !== 'unknown' && currentCoverage !== 'unknown') {
              const targetPercent = parseInt(targetCoverage.replace('%', ''));
              const currentPercent = parseInt(currentCoverage);
              const difference = currentPercent - targetPercent;

              if (difference > 0) {
                changeEmoji = 'ğŸ“ˆ';
                comparisonMessage = `
            ## ${changeEmoji} Coverage Improved!

            | Branch | Coverage | Change |
            |--------|----------|--------|
            | **${targetBranch}** (target) | **${targetCoverage}** | baseline |
            | **Current PR** | **${currentCoverage}%** | **+${difference}%** âœ… |

            ğŸ‰ **Great job!** This PR increases code coverage by **${difference} percentage point${difference !== 1 ? 's' : ''}**.`;

              } else if (difference < 0) {
                changeEmoji = 'ğŸ“‰';
                comparisonMessage = `
            ## ${changeEmoji} Coverage Decreased

            | Branch | Coverage | Change |
            |--------|----------|--------|
            | **${targetBranch}** (target) | **${targetCoverage}** | baseline |
            | **Current PR** | **${currentCoverage}%** | **${difference}%** âš ï¸ |

            âš ï¸ **Attention:** This PR decreases code coverage by **${Math.abs(difference)} percentage point${Math.abs(difference) !== 1 ? 's' : ''}**.
            Consider adding more tests to maintain or improve coverage.`;

              } else {
                changeEmoji = 'ğŸ”„';
                comparisonMessage = `
            ## ${changeEmoji} Coverage Maintained

            | Branch | Coverage | Change |
            |--------|----------|--------|
            | **${targetBranch}** (target) | **${targetCoverage}** | baseline |
            | **Current PR** | **${currentCoverage}%** | **no change** âœ… |

            âœ… **Perfect!** This PR maintains the same coverage level as the target branch.`;
              }
            } else {
              comparisonMessage = `
            ## ğŸ“Š Coverage Report

            **Current PR Coverage:** ${currentCoverage}%

            > ğŸ’¡ **Note:** Could not compare with ${targetBranch} branch coverage.
            This might be the first run or target branch is not accessible.`;
            }

            const body = `${comparisonMessage}

            ${coverageDetails}

            ---

            ğŸ“‹ **Coverage Target Guidelines:**
            - ğŸ¯ **Target:** Maintain or improve coverage
            - âœ… **Good:** Above 80% coverage
            - âš ï¸ **Acceptable:** 70-80% coverage
            - âŒ **Needs Work:** Below 70% coverage

            > ğŸ”— View detailed coverage report in the main coverage comment above`;

            // Find existing coverage comparison comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const comparisonComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              (comment.body.includes('Coverage Improved') ||
               comment.body.includes('Coverage Decreased') ||
               comment.body.includes('Coverage Maintained') ||
               comment.body.includes('Coverage Report'))
            );

            if (comparisonComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: comparisonComment.id,
                body: body
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

      - name: Upload coverage to Codecov
        if: always()
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage.xml
          flags: unittests
          name: codecov-${{ matrix.python-version }}
          fail_ci_if_error: false
          verbose: true
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

      - name: Test Report
        if: always()
        uses: dorny/test-reporter@v1
        continue-on-error: true
        with:
          name: Test Results (Python ${{ matrix.python-version }})
          path: test-results.xml
          reporter: java-junit
          fail-on-error: false
          path-replace-backslashes: false
          list-suites: all
          list-tests: failed
          max-annotations: 10

      - name: Enhanced Test Results Comment
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            // Get test summary
            let testSummary = 'âœ… All tests passed!';
            let statusEmoji = 'âœ…';

            if ('${{ steps.test.outputs.test_failed }}' === 'true') {
              testSummary = 'âŒ Some tests failed';
              statusEmoji = 'âŒ';
            }

            // Parse test results with enhanced details
            let testDetails = '';
            let executionTime = '';

            try {
              if (fs.existsSync('test-results.xml')) {
                const xmlContent = fs.readFileSync('test-results.xml', 'utf8');
                const testsMatch = xmlContent.match(/tests="(\d+)"/);
                const failuresMatch = xmlContent.match(/failures="(\d+)"/);
                const errorsMatch = xmlContent.match(/errors="(\d+)"/);
                const skippedMatch = xmlContent.match(/skipped="(\d+)"/);
                const timeMatch = xmlContent.match(/time="([^"]+)"/);

                if (testsMatch) {
                  const tests = parseInt(testsMatch[1]);
                  const failures = failuresMatch ? parseInt(failuresMatch[1]) : 0;
                  const errors = errorsMatch ? parseInt(errorsMatch[1]) : 0;
                  const skipped = skippedMatch ? parseInt(skippedMatch[1]) : 0;
                  const passed = tests - failures - errors - skipped;

                  if (timeMatch) {
                    const timeSeconds = parseFloat(timeMatch[1]);
                    executionTime = timeSeconds > 60
                      ? `â±ï¸ **Execution Time:** ${Math.floor(timeSeconds/60)}m ${Math.round(timeSeconds%60)}s`
                      : `â±ï¸ **Execution Time:** ${timeSeconds.toFixed(2)}s`;
                  }

                  const passRate = ((passed / tests) * 100).toFixed(1);

                  testDetails = `
            ## ğŸ§ª Test Execution Summary

            | Status | Count | Percentage |
            |--------|-------|------------|
            | âœ… **Passed** | **${passed}** | **${passRate}%** |
            | âŒ Failed | ${failures} | ${((failures/tests)*100).toFixed(1)}% |
            | ğŸ”¥ Errors | ${errors} | ${((errors/tests)*100).toFixed(1)}% |
            | â­ï¸ Skipped | ${skipped} | ${((skipped/tests)*100).toFixed(1)}% |
            | ğŸ“ **Total** | **${tests}** | **100%** |

            ${executionTime}`;
                }
              }
            } catch (e) {
              console.log('Could not parse test results:', e);
              testDetails = '\nğŸ” Test details could not be parsed from XML output.';
            }

            const body = `## ${statusEmoji} Test Execution Report

            ### Python ${{ matrix.python-version }} â€¢ ${testSummary}

            ${testDetails}

            ---

            ğŸ“‹ **Workflow Details:**
            - ğŸ **Python Version:** ${{ matrix.python-version }}
            - ğŸ–¥ï¸ **Runner OS:** ${{ runner.os }}
            - ğŸ“ **Commit:** \`${{ github.sha }}\`
            - ğŸ”— [**View Full Logs**](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})

            > ğŸ’¡ **Note:** Detailed coverage report available in separate comment below`;

            // Find existing test comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const testComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('Test Execution Report')
            );

            if (testComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: testComment.id,
                body: body
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

      - name: Check test status
        if: steps.test.outputs.test_failed == 'true'
        run: |
          echo "âŒ Tests failed! Please fix the failing tests before merging."
          exit 1

  lint:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: 'pip'

      - name: Install linting dependencies
        run: |
          python -m pip install --upgrade pip
          pip install ruff mypy

      - name: Run Ruff linter
        id: ruff
        run: |
          ruff check src/ tests/ --output-format=github || RUFF_FAILED=$?
          if [ "${RUFF_FAILED}" != "" ]; then
            echo "ruff_failed=true" >> $GITHUB_OUTPUT
          else
            echo "ruff_failed=false" >> $GITHUB_OUTPUT
          fi

      - name: Run Ruff formatter check
        id: ruff-format
        run: |
          ruff format --check src/ tests/ || FORMAT_FAILED=$?
          if [ "${FORMAT_FAILED}" != "" ]; then
            echo "format_failed=true" >> $GITHUB_OUTPUT
            echo "âŒ Code formatting issues found. Run 'ruff format src/ tests/' to fix."
          else
            echo "format_failed=false" >> $GITHUB_OUTPUT
          fi

      - name: Comment lint results on PR
        if: github.event_name == 'pull_request' && (steps.ruff.outputs.ruff_failed == 'true' || steps.ruff-format.outputs.format_failed == 'true')
        uses: actions/github-script@v7
        with:
          script: |
            let message = '## âš ï¸ Code Quality Issues Found\n\n';
            
            if ('${{ steps.ruff.outputs.ruff_failed }}' === 'true') {
              message += 'âŒ **Linting issues detected**\n';
              message += 'Run `ruff check src/ tests/ --fix` to auto-fix some issues.\n\n';
            }
            
            if ('${{ steps.ruff-format.outputs.format_failed }}' === 'true') {
              message += 'âŒ **Formatting issues detected**\n';
              message += 'Run `ruff format src/ tests/` to fix formatting.\n\n';
            }
            
            message += 'ğŸ“š [View workflow logs](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})';
            
            github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: message
            });

  all-checks-passed:
    name: All Checks Passed
    needs: [test, lint]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: Check results
        run: |
          if [ "${{ needs.test.result }}" != "success" ] || [ "${{ needs.lint.result }}" != "success" ]; then
            echo "âŒ Some checks failed!"
            echo "Test result: ${{ needs.test.result }}"
            echo "Lint result: ${{ needs.lint.result }}"
            exit 1
          else
            echo "âœ… All checks passed successfully!"
          fi