name: Test Suite

on:
  pull_request:
    branches: [main, dev]
  push:
    branches: [dev]
  workflow_dispatch:  # Allow manual trigger

jobs:
  test:
    name: Python ${{ matrix.python-version }} Tests
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        python-version: ["3.12"]
      fail-fast: false
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -e ".[test,discord]"
          pip install pytest-rerunfailures
          
      - name: Set up test environment
        run: |
          # Create test database
          mkdir -p data
          touch data/test.db
          
          # Set environment variables for testing
          echo "DATABASE_URL=sqlite:///data/test.db" >> $GITHUB_ENV
          echo "AUTOMAGIK_OMNI_API_KEY=${{ secrets.TEST_API_KEY || 'dummy-key' }}" >> $GITHUB_ENV
          echo "PYTHONPATH=${{ github.workspace }}" >> $GITHUB_ENV

      - name: Run tests
        id: pytest
        run: |
          # Run pytest with simple output, skip known CI issues, retry flaky tests
          python -m pytest tests/ \
            -v \
            --tb=short \
            --color=yes \
            --no-header \
            -rN \
            --maxfail=10 \
            --reruns 2 \
            --reruns-delay 1 \
            -k "not test_bearer_token_validation and not test_add_command_help" \
            2>&1 | tee test-output.txt

          # Capture exit code
          PYTEST_EXIT_CODE=${PIPESTATUS[0]}

          # Extract test summary
          echo "## Test Summary" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          tail -n 20 test-output.txt >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY

          # Exit with pytest exit code
          exit $PYTEST_EXIT_CODE

      - name: Upload test output
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-output-${{ matrix.python-version }}
          path: test-output.txt
          retention-days: 7

  test-ui:
    name: UI Tests (Playwright)
    runs-on: ubuntu-latest
    needs: test

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: resources/ui/package-lock.json

      - name: Install UI dependencies
        working-directory: resources/ui
        run: |
          echo "Current directory: $(pwd)"
          ls -la package*.json || echo "No package files found"
          # Use npm install instead of npm ci because parent dir has package.json without lock file
          npm install --legacy-peer-deps --ignore-scripts

      - name: Install Playwright browsers
        working-directory: resources/ui
        run: npx playwright install chromium --with-deps

      - name: Set up Python for backend
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: 'pip'

      - name: Install backend dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -e .

      - name: Start backend services
        run: |
          mkdir -p data
          export DATABASE_URL=sqlite:///data/test.db
          export AUTOMAGIK_OMNI_API_KEY=test-api-key-for-playwright
          export TEST_API_KEY=test-api-key-for-playwright
          python -c "
          from sqlalchemy import create_engine
          from src.db.models import Base
          import os
          engine = create_engine(os.environ['DATABASE_URL'])
          Base.metadata.create_all(engine)
          "
          # Start backend in background
          nohup python -m uvicorn src.api.app:app --host 0.0.0.0 --port 8882 > backend.log 2>&1 &
          echo $! > backend.pid
          # Wait for backend to be ready
          for i in {1..30}; do
            if curl -s http://localhost:8882/health > /dev/null; then
              echo "Backend is ready"
              break
            fi
            echo "Waiting for backend... ($i/30)"
            sleep 2
          done

      - name: Run Playwright tests (headless)
        working-directory: resources/ui
        run: npm run test:e2e
        env:
          CI: true
          TEST_API_KEY: test-api-key-for-playwright

      - name: Upload Playwright test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: playwright-report
          path: resources/ui/playwright-report/
          retention-days: 7

      - name: Upload Playwright test videos
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: playwright-videos
          path: resources/ui/test-results/
          retention-days: 7

      - name: Stop backend services
        if: always()
        run: |
          if [ -f backend.pid ]; then
            kill $(cat backend.pid) || true
            rm backend.pid
          fi

  lint:
    name: Code Quality
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install ruff
        run: |
          python -m pip install --upgrade pip
          pip install ruff

      - name: Run ruff check
        id: ruff-check
        continue-on-error: true
        run: |
          echo "## Ruff Linting Results" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          ruff check src/ tests/ 2>&1 | tee -a $GITHUB_STEP_SUMMARY || true
          echo '```' >> $GITHUB_STEP_SUMMARY

      - name: Run ruff format check
        id: ruff-format
        continue-on-error: true
        run: |
          echo "## Ruff Format Check" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          ruff format --check src/ tests/ 2>&1 | tee -a $GITHUB_STEP_SUMMARY || true
          echo '```' >> $GITHUB_STEP_SUMMARY

  status:
    name: Check Status
    needs: [test, test-ui, lint]
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Determine status
        run: |
          echo "## Workflow Status" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ needs.test.result }}" == "success" ]; then
            echo "✅ **Tests:** Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Tests:** Failed" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.test-ui.result }}" == "success" ]; then
            echo "✅ **UI Tests:** Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **UI Tests:** Failed" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.lint.result }}" == "success" ]; then
            echo "✅ **Linting:** Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ **Linting:** Failed (non-blocking)" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY

          # Fail if tests failed
          if [ "${{ needs.test.result }}" != "success" ] || [ "${{ needs.test-ui.result }}" != "success" ]; then
            echo "❌ Tests must pass before merging!" >> $GITHUB_STEP_SUMMARY
            exit 1
          else
            echo "✅ All required checks passed!" >> $GITHUB_STEP_SUMMARY
          fi